<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="#">
    <meta name="keywords" content="#">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>V-MORALS</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <style>
        /* Custom style for video captions */
        .video-caption {
            font-size: 0.9rem;
            color: #555;
            text-align: center;
            margin-top: 0.5rem;
            font-style: italic;
        }
    </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">V-MORALS: Visual Morse Graph-Aided Discovery
of Regions of Attraction in a Learned Space</h1>
                        <div class="is-size-5 publication-authors"></div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                            <a href="https://www.linkedin.com/in/faiz-aladin/">Faiz Aladin</a><sup>1</sup>,</span>
                            <span class="author-block">
                            <a href="https://www.linkedin.com/in/balasubramanian-ashwin/">Ashwin Balasubramanian</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://sites.google.com/view/larslindemann/main-page">Lars Lindemann</a><sup>1,2</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://danielseita.github.io/">Daniel Seita</a><sup>1</sup>,
                            </span>
                        </div>
                        <div class="is-size-6 publication-authors">
                            <span class="author-block"><sup>1</sup>University of Southern California,</span>
                            <span class="author-block"><sup>2</sup>ETH Zurich</span>
                        </div>
                        <div class = "is-size-5 publication-authors">
                            International Conference on Robotics and Automation (ICRA) 2026
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="./static/pdfs/V-MORALS.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/slurm-lab-usc/V-MORALS" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
               <div class="publication-video">
                        <video controls="" muted="" height="80% !important" width="80% !important">
                            <source src="./static/videos/V-MORALS.mp4" type="video/mp4">
                        </video>
                    </div>
            </div>
        </div>
    </section>

    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Reachability analysis has become increasingly important in robotics to distinguish safe from unsafe states. Unfortunately, existing reachability and safety analysis methods often fall short, as they require known system dynamics or large datasets to estimate accurate system models, are computationally expensive, and assume full state information. Existing work such as MORALS aims to solve this by using topological tools to estimate Regions of Attraction for a controller in a low-dimensional latent space. However, MORALS still relies on full state knowledge and has not been studied when only sensor measurements are available. This paper presents Visual Morse Graph-Aided Discovery of Regions of Attraction in a Learned Space (V-MORALS). V-MORALS takes in a dataset of image- based trajectories of a system under a given controller, and defines a learned latent space for reachability analysis. Using the learned latent space, our method is able to generate well-defined Morse graphs and ROAs for various systems. V-MORALS provides capabilities similar to the original MORALS architecture without relying on state knowledge, and using only high level sensor data.                        
                        </p>
                    </div>
                </div>
            </div>
    </section>

<section class="section" id="Overview">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Method Overview</h2>

    <!-- High-level pipeline -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img
          src="./static/imgs/method_diagram.png"
          alt="V-MORALS method overview"
          style="max-width: 100%; height: auto;"
        />
        <p class="has-text-grey is-size-6 mt-2">
          End-to-end pipeline of V-MORALS: from visual trajectories to Morse graphs and regions of attraction.
        </p>
      </div>
    </div>

    <!-- Step-by-step explanation -->
    <div class="content has-text-justified mt-5">
      <p>
        <strong>V-MORALS</strong> performs reachability and safety analysis directly from visual observations,
        without access to system dynamics or full state information. Given image-based trajectories collected
        under a fixed controller, the method proceeds in three main stages:
      </p>

      <ol>
        <li>
          <strong>Latent Representation Learning.</strong>
          Short sequences of images are encoded using a <em>3D convolutional autoencoder</em>, producing a
          compact latent space that captures the system’s underlying dynamics while mitigating partial
          observability.
        </li>

        <li>
          <strong>Dynamics Modeling.</strong>
          Transitions between latent states are learned from the encoded trajectories, forming a discrete
          approximation of the system’s behavior in the learned space.
        </li>

        <li>
          <strong>Morse Graph Construction and ROA Discovery.</strong>
          The latent space is discretized to construct a <em>Morse graph</em>, which summarizes global system
          behavior. Attractors and their associated <em>Regions of Attraction (ROAs)</em> are then identified
          directly from this graph.
        </li>
      </ol>
      <p>
        By operating entirely on image-based data, V-MORALS extends the original MORALS framework to
        vision-only settings and enables interpretable reachability analysis from high-dimensional sensory inputs.
      </p>
    </div>

    <!-- Component visuals -->
    <div class="columns is-centered mt-5">
      <div class="column is-half has-text-centered">
        <img
          src="./static/imgs/encoder.png"
          alt="Spatiotemporal encoder architecture"
          style="max-width: 100%; height: auto;"
        />
        <p class="has-text-grey is-size-6 mt-2">
          Encoder used to learn a compact latent representation from image sequences.
        </p>
      </div>

      <div class="column is-half has-text-centered">
        <img
          src="./static/imgs/morse_graph.png"
          alt="Morse graph in latent space"
          style="max-width: 100%; height: auto;"
        />
        <p class="has-text-grey is-size-6 mt-2">
          Morse graph constructed in the learned latent space, revealing attractors and regions of attraction.
        </p>
      </div>
    </div>

  </div>
</section>


    <section class="section" style="padding-top: 1rem; padding-bottom: 1rem;" id="Results">
        <div class="container is-max-desktop content">
            <h2 class="title is-3">Video Rollouts</h2>

            <div class="publication-video">
                <video controls="" muted="" height="100% !important" width="100% !important">
                    <source src="./static/videos/humanoid_trajectory_12_34_merged_3d_sped.mp4" type="video/mp4">
                </video>
                <p class="video-caption">
                    This is the trajectory rollout of a humanoid in 3D latent space: the square marks the start position, the circle shows the current position, and the triangle indicates the goal
                </p>
            </div>


            <div class="publication-video">
                <video controls="" muted="" height="100% !important" width="100% !important">
                    <source src="./static/videos/humanoid_trajectory_12_34_merged_final.mp4" type="video/mp4">
                </video>
                <p class="video-caption">
                    This is a trajectory rollout of a humanoid within a latent space of 2 dimensions. 
                    We see that the final states are located in different regions.
                </p>
            </div>

            <div class="publication-video">
                <video controls="" muted="" height="100% !important" width="100% !important">
                    <source src="./static/videos/pendulum_trajectory_30_32_merged.mp4" type="video/mp4">
                </video>
                <p class="video-caption">
                    This is a trajectory rollout of a Pendulum within a latent space of 2 dimensions. 
                    Although both trajectories start in the same region (due to similar positions), 
                    we see that the final states are in distinct regions.
                </p>
            </div>

            <div class="publication-video">
                <video controls="" muted="" height="100% !important" width="100% !important">
                    <source src="./static/videos/cartpole_trajectory_11_10_merged_fast.mp4" type="video/mp4">
                </video>
                <p class="video-caption">
                    This is a trajectory rollout of a CartPole within a latent space of 2 dimensions.
                </p>
            </div>

        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>TBD</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website template is borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
</html>
